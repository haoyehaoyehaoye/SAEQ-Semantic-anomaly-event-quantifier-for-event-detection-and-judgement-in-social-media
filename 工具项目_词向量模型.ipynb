{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_270949/1085421068.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from imp import reload\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import models,corpora\n",
    "from gensim.models import doc2vec, ldamodel\n",
    "from gensim.models import CoherenceModel\n",
    "import random\n",
    "import warnings\n",
    "from math import log, exp\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadTweets(dataname):\n",
    "    doclist=[]\n",
    "    if dataname=='足球':\n",
    "       with open('./推特足球/tweets_football.csv',encoding=\"utf-8\") as f:\n",
    "         f_csv = csv.reader(f)\n",
    "         pos = 1\n",
    "         for row in f_csv: \n",
    "            if pos<10000:  #控制聚类文本数目\n",
    "                #print(row[3])\n",
    "                doclist.append(row[3])\n",
    "            pos+=1\n",
    "         print(f'总数量{pos}')\n",
    "       return doclist\n",
    "    elif dataname=='比特币':\n",
    "       with open('./推特比特币/cleanprep.csv',encoding=\"utf-8\") as f:\n",
    "         f_csv = csv.reader(f)\n",
    "         pos = 1\n",
    "         for row in f_csv: \n",
    "            if pos<100000:  #控制聚类文本数目\n",
    "                #print(row[2])\n",
    "                if '\\\\x' not in row[2]: #只取英文数据\n",
    "                    doclist.append(row[2])\n",
    "            pos+=1\n",
    "         print(f'总数量{pos}')\n",
    "       return doclist\n",
    "    elif dataname=='比特币2': #一个更大的数据集\n",
    "       with open('./推特比特币2/Bitcoin_tweets.csv',encoding=\"utf-8\") as f:\n",
    "         f_csv = csv.reader(f)\n",
    "         pos = 1\n",
    "         for row in f_csv:\n",
    "            #print(len(row))\n",
    "            #print(row)\n",
    "            if  len(row)>=10:  #控制聚类文本数目并处理缺失数据,全部跑太多了跑不了\n",
    "                #print(row[8])\n",
    "                doclist.append(row[9])\n",
    "                pos+=1\n",
    "         print(f'总数量{pos}')\n",
    "       return random.sample(doclist,100000)  #从列表中随机抽取元素\n",
    "    elif dataname=='GPT': #一个更大的数据集\n",
    "       with open('./推特GPT/GPT.csv',encoding=\"utf-8\") as f:\n",
    "         f_csv = csv.reader(f)\n",
    "         pos = 1\n",
    "         for row in f_csv:\n",
    "            #print(len(row))\n",
    "            #print(row)\n",
    "            if  len(row)>=6:  #控制聚类文本数目并处理缺失数据,全部跑太多了跑不了\n",
    "                #print(row[0])\n",
    "                doclist.append(row[2])\n",
    "                pos+=1\n",
    "         print(f'总数量{pos}')\n",
    "       return doclist  #从列表中随机抽取元素\n",
    "    else:\n",
    "        print('无此数据源')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练词向量模型的，理论上只需要运行一次\n",
    "def WordModelTrain():\n",
    "    raw_sentences=ReadTweets('GPT')\n",
    "    print(f'总数量{len(raw_sentences)}')\n",
    "    sentences = [s.split() for s in raw_sentences]\n",
    "    model = Word2Vec(sentences,vector_size = 30, window = 2 , min_count = 3, epochs=7, negative=10,sg=1)\n",
    "    model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数量1010242\n"
     ]
    }
   ],
   "source": [
    "doclist=[]\n",
    "src='../一般数据集验证/en_US.news.txt'\n",
    "with open(src, 'r') as file:\n",
    "        content = file.readlines()\n",
    "        for item in content:\n",
    "            doclist.append(item)\n",
    "print(f'总数量{len(doclist)}')\n",
    "sentences = [s.split() for s in doclist]\n",
    "model = Word2Vec(sentences,vector_size = 30, window = 5 , min_count = 2, epochs=7, negative=10,sg=1)\n",
    "model.save('word2vec_news_data.model')\n",
    "\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
