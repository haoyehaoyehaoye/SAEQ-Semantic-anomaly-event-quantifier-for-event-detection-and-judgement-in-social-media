{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from imp import reload\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import models,corpora\n",
    "from gensim.models import doc2vec, ldamodel\n",
    "from gensim.models import CoherenceModel\n",
    "import random\n",
    "import warnings\n",
    "from keybert import KeyBERT\n",
    "from math import log, exp\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadTweetsByTime(dataname,date_list):\n",
    "    doclist=[]\n",
    "    if dataname=='GPT': #搜集的推特关于GPT的数据集，时间从一月四号持续到三月29号\n",
    "       with open('../推特GPT/GPT.csv',encoding=\"utf-8\") as f:\n",
    "         f_csv = csv.reader(f)\n",
    "         pos = 1\n",
    "         for row in f_csv:\n",
    "            if  len(row)>=6:  #控制聚类文本数目并处理缺失数据,全部跑太多了跑不了\n",
    "                #print(row[0][:10])  #取到日期\n",
    "                if row[0][:10] in date_list: #判断时间\n",
    "                    #print(row[0])\n",
    "                    doclist.append(row[2])\n",
    "                    pos+=1\n",
    "         print(f'总数量{pos}')\n",
    "    #return random.sample(doclist,200)\n",
    "    global data_number\n",
    "    if data_number!=0:\n",
    "        return random.sample(doclist,data_number)\n",
    "    else:  #表示数据全部要\n",
    "        data_number=len(doclist)\n",
    "        return doclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetKeyBertWord(dataname,date_list):\n",
    "    #使用keybert方法，获取关键词\n",
    "    kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "    doclist=ReadTweetsByTime(dataname,date_list)\n",
    "    dic={}\n",
    "    list_word=[]\n",
    "    list_weight=[]\n",
    "    pos=0\n",
    "    for item in doclist:\n",
    "        #print(item)\n",
    "        pos=pos+1\n",
    "        global data_number\n",
    "        #print(pos/data_number)\n",
    "        print(\"\\r\", end=\"\")\n",
    "        print(\"取词中: {}\".format(pos/data_number),end=\"\") #进度条\n",
    "        #有点慢，两千条数据要跑十分钟\n",
    "        keywords_phrase = kw_model.extract_keywords(item, \n",
    "                                     keyphrase_ngram_range=(1,1),   #这个就是范围,可以考虑词组,本实验暂时只考虑词\n",
    "                                     stop_words='english', \n",
    "                                     highlight=False, \n",
    "                                     top_n=3) \n",
    "        keywords_phrase_list= list(keywords_phrase)#结果，返回的是列表包含元组的形式[(),()]\n",
    "        #print(keywords_phrase_list)\n",
    "        \n",
    "        for word in keywords_phrase_list:\n",
    "            #print(word[0])\n",
    "            #把数据加入字典,要处理1.去除全局的词  2.判断是否已经存在于字典中\n",
    "            if word[0] not in ['chatgpt','gpt','#chatgpt','chat']:\n",
    "              if word[0] not in list_word:\n",
    "                list_word.append(word[0])\n",
    "                list_weight.append(word[1])\n",
    "              else:\n",
    "                index=list_word.index(word[0])\n",
    "                list_weight[index]=list_weight[index]+word[1]\n",
    "        #print(keywords_list)\n",
    "        \n",
    "    #最终返回两个数据，字典形式的结果和原始数据\n",
    "    dic['word']=list_word\n",
    "    dic['weight']=list_weight\n",
    "    print(' ')#这个空行主要是为了和进度条配合完成换行,后面也有\n",
    "    return dic,doclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBrim(list_word):\n",
    "    #获取节点间的边信息，主要使用相似度\n",
    "    list_link_dic=[]\n",
    "    global threshold_similarity  #全局变量，边的门限\n",
    "    model = models.word2vec.Word2Vec.load('word2vec.model')  #使用相似度试试\n",
    "    for i in range(0,len(list_word)):\n",
    "        dic={}\n",
    "        for j in range(i+1,len(list_word)):\n",
    "            try:\n",
    "               similarity = model.wv.similarity(list_word[i],list_word[j])\n",
    "            except:\n",
    "               similarity=0   #主要是不存在某个词的情况\n",
    "            if(similarity>=threshold_similarity):  #判断点的权重\n",
    "                dic={}\n",
    "                dic['Source']=i\n",
    "                dic['Target']=j\n",
    "                dic['Type']=0\n",
    "                dic['Weight']=similarity\n",
    "                list_link_dic.append(dic)\n",
    "        #print(f\"\\r算边中:{(i+1)/len(list_word)}\",end=' ')\n",
    "        print(\"\\r\", end=\"\")\n",
    "        print(f'算边进度为{(i+1)/len(list_word)}',end=\"\",flush=True)\n",
    "    print('\\n完成算边')\n",
    "    return list_link_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteCSV_gephi(file_path,time_name,list_word_dic,list_link_dic):#写两个csv，在gephi中完成画图\n",
    "    #具体格式，第一个节点csv，依次为'id','label','type','typelabel','size'\n",
    "    #          第二个连线csv 依次为'源','目标','Label','Id','Weight'\n",
    "    global write_version\n",
    "    file_node=file_path+write_version+time_name+'-node.csv'\n",
    "    file_link=file_path+write_version+time_name+'-link.csv'\n",
    "    list_node=[]\n",
    "    list_link=[]\n",
    "    #print('开始写入文件')\n",
    "    for item in list_word_dic:\n",
    "        node=[]\n",
    "        node.append(item['pos'])\n",
    "        node.append(item['word'])\n",
    "        node.append('1') #暂时不分type\n",
    "        node.append('1') #暂时不分type\n",
    "        node.append(item['weight'])\n",
    "        list_node.append(node)\n",
    "    for item in list_link_dic:\n",
    "        link=[]\n",
    "        link.append(item['Source'])\n",
    "        link.append(item['Target'])\n",
    "        link.append(item['Type'])\n",
    "        link.append(item['Weight'])\n",
    "        list_link.append(link)\n",
    "    #在写入边的时候进行判断，舍弃低权边\n",
    "    #list_link.sort(key=lambda ele: ele[3], reverse=True)#降序\n",
    "    #list_link=list_link[0:300]  #取前多少条\n",
    "    with open(file_node, 'w', encoding='UTF8',newline=\"\") as f_node:\n",
    "        writer = csv.writer(f_node) \n",
    "        writer.writerow(['id','label','type','typelabel','size'])\n",
    "        for item in list_node:\n",
    "            writer.writerow(item)\n",
    "    with open(file_link, 'w', encoding='UTF8',newline=\"\") as f_link:\n",
    "        writer = csv.writer(f_link) \n",
    "        writer.writerow(['Source','Target','Type','Weight'])\n",
    "        for item in list_link:\n",
    "            writer.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNode(sorted_word,sorted_weight):\n",
    "    #工具函数，将词和权重打包\n",
    "    list_word_dic=[]\n",
    "    for i in range(0,len(sorted_word)):\n",
    "        dic={}\n",
    "        dic['pos']=i\n",
    "        dic['word']=sorted_word[i]\n",
    "        dic['weight']=sorted_weight[i]\n",
    "        list_word_dic.append(dic)\n",
    "    return list_word_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: DeprecationWarning: invalid escape sequence '\\c'\n",
      "<>:6: DeprecationWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\12193\\AppData\\Local\\Temp\\ipykernel_14228\\371125812.py:6: DeprecationWarning: invalid escape sequence '\\c'\n",
      "  filepath_name='E:\\code\\舆情idea2\\推特GPT'#文件夹根目录位置\n"
     ]
    }
   ],
   "source": [
    "threshold_similarity=0.8   #成边的相似度 门限值\n",
    "data_number=0  #这个值是考虑的文本数量，如果为0则是全部文本。否则只取data_number条。  这个参数也帮助进度条\n",
    "write_version='/成图csvV3/'  #控制最终结果写入那个文件夹\n",
    "random.seed(100)\n",
    "source_data='GPT' #数据源\n",
    "filepath_name='E:\\code\\舆情idea2\\推特GPT'#文件夹根目录位置\n",
    "model = models.word2vec.Word2Vec.load('word2vec.model')\n",
    "split_number=50   #截取前多少个词\n",
    "#需要操作的日期\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_date=[\n",
    "    #{'time':'1月6日起','date':['2023-01-06','2023-01-07','2023-01-08'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    #{'time':'1月9日起','date':['2023-01-09','2023-01-10','2023-01-11'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    #{'time':'1月12日起','date':['2023-01-12','2023-01-13','2023-01-14'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    #{'time':'1月15日起','date':['2023-01-15','2023-01-16','2023-01-17'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    #{'time':'1月18日起','date':['2023-01-18','2023-01-19','2023-01-20'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    #{'time':'1月21日起','date':['2023-01-21','2023-01-22','2023-01-23'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    #{'time':'1月24日起','date':['2023-01-24','2023-01-25','2023-01-26'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    #{'time':'1月27日起','date':['2023-01-27','2023-01-28','2023-01-29'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    #{'time':'1月30日起','date':['2023-01-30','2023-01-31','2023-02-01'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    #{'time':'2月2日起','date':['2023-02-02','2023-02-03','2023-02-04'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'2月5日起','date':['2023-02-05','2023-02-06','2023-02-07'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'2月8日起','date':['2023-02-08','2023-02-09','2023-02-10'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'2月11日起','date':['2023-02-11','2023-02-12','2023-02-13'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'2月14日起','date':['2023-02-14','2023-02-15','2023-02-16'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'2月17日起','date':['2023-02-17','2023-02-18','2023-02-19'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'2月20日起','date':['2023-02-20','2023-02-21','2023-02-22'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'2月23日起','date':['2023-02-23','2023-02-24','2023-02-25'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'2月26日起','date':['2023-02-26','2023-02-27','2023-02-28'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'3月1日起','date':['2023-03-01','2023-03-02','2023-03-03'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'3月4日起','date':['2023-03-04','2023-03-05','2023-03-06'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'3月7日起','date':['2023-03-07','2023-03-08','2023-03-09'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'3月10日起','date':['2023-03-10','2023-03-11','2023-03-12'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'3月13日起','date':['2023-03-13','2023-03-14','2023-03-15'],'tips':'全数据，边为相似度,门限0 .8'},\n",
    "    {'time':'3月16日起','date':['2023-03-16','2023-03-17','2023-03-18'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'3月19日起','date':['2023-03-19','2023-03-20','2023-03-21'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'3月22日起','date':['2023-03-22','2023-03-23','2023-03-24'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'3月25日起','date':['2023-03-25','2023-03-26','2023-03-27'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "    {'time':'3月28日起','date':['2023-03-28','2023-03-29'],'tips':'全数据，边为相似度,门限0.8'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始执行:2月5日起\n",
      "总数量22807\n",
      "取词中: 1.09995615188985354444 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:2月8日起\n",
      "总数量24369\n",
      "取词中: 1.09995896257386745666 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:2月11日起\n",
      "总数量17815\n",
      "取词中: 1.0 994386437633326456\n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:2月14日起\n",
      "总数量18674\n",
      "取词中: 1.09994644674128423654 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:2月17日起\n",
      "总数量16116\n",
      "取词中: 1.09993794601303136466 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:2月20日起\n",
      "总数量17955\n",
      "取词中: 1.09994430210538043456 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:2月23日起\n",
      "总数量15077\n",
      "取词中: 1.09993366940833123563 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:2月26日起\n",
      "总数量13957\n",
      "取词中: 1.09992834623101183653 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月1日起\n",
      "总数量17548\n",
      "取词中: 1.09994301020117497566 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月4日起\n",
      "总数量13556\n",
      "取词中: 1.0 992622648469296555\n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月7日起\n",
      "总数量15354\n",
      "取词中: 1.09993486614993815546 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月10日起\n",
      "总数量12913\n",
      "取词中: 1.09992255266418844665 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月13日起\n",
      "总数量25127\n",
      "取词中: 1.09996020058903127457 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月16日起\n",
      "总数量25264\n",
      "取词中: 1.0 996041641926923653\n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月19日起\n",
      "总数量20432\n",
      "取词中: 1.09995105476971274564 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月22日起\n",
      "总数量24490\n",
      "取词中: 1.09995916533954023546 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月25日起\n",
      "总数量19777\n",
      "取词中: 1.09994943365695793556 \n",
      "算边进度为1.08\n",
      "完成算边\n",
      "开始执行:3月28日起\n",
      "总数量16430\n",
      "取词中: 1.0 993913202264284654\n",
      "算边进度为1.08\n",
      "完成算边\n"
     ]
    }
   ],
   "source": [
    "for item in ls_date:\n",
    "    #不知道为什么，print \\r实现的进度条会超过一点\n",
    "    print(f\"开始执行:{item['time']}\")\n",
    "    data_number=0  #这个值是考虑的文本数量，如果为0则是全部文本。否则只取data_number条。  这个参数也帮助进度条\n",
    "    dic,doclist = GetKeyBertWord(source_data,item['date'])\n",
    "    sorted_word = sorted(dic['word'], key=lambda x: dic['weight'][dic['word'].index(x)],reverse=True)  #根据weight操作word排序\n",
    "    sorted_weight=sorted(dic['weight'],reverse=True)\n",
    "    list_link_dic=GetBrim(sorted_word[:split_number])  #数据越多越慢，不建议超过两千条  #只取权重在前面的词\n",
    "    list_word_dic=GetNode(sorted_word[:split_number],sorted_weight[:split_number]) #拼接出词\n",
    "    WriteCSV_gephi(filepath_name,item['time']+str(data_number)+item['tips'],list_word_dic,list_link_dic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
