{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community #  \n",
    "import torch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "import random\n",
    "from keybert import KeyBERT\n",
    "import time\n",
    "from gensim import models,corpora\n",
    "from math import log, exp\n",
    "import json\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import models,corpora\n",
    "from gensim.models import doc2vec, ldamodel\n",
    "from gensim.models import CoherenceModel\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import time\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(datelist):\n",
    "    doclist=[]\n",
    "    src='../../../推特GPT/GPT.csv'\n",
    "    with open(src,encoding=\"utf-8\") as f:\n",
    "         f_csv = csv.reader(f)\n",
    "         pos = 1\n",
    "         for row in f_csv:\n",
    "            if  len(row)>=6:  #控制聚类文本数目并处理缺失数据,全部跑太多了跑不了\n",
    "                #print(row[0][:10])  #取到日期\n",
    "                if row[0][:10] in datelist: #判断时间\n",
    "                    #print(row[0])\n",
    "                    dic={}\n",
    "                    dic['like_count']=eval(row[4])\n",
    "                    dic['retweet_count']=eval(row[5])\n",
    "                    dic['data'] = row[2]\n",
    "                    doclist.append(dic)\n",
    "                    pos+=1                 \n",
    "    return doclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#工具函数 计算pmi\n",
    "#已经改成了计算频率\n",
    "def GetPointwiseMutualValue(word_1,word_2,doclist):\n",
    "    value_1=0\n",
    "    value_2=0\n",
    "    value_mutual=0\n",
    "    count_word=1  #一共出现多少词,加一不影响结果，只是防止除0\n",
    "    for item in doclist:\n",
    "        item = item['data']\n",
    "        if word_1 in item: \n",
    "            value_1+=1\n",
    "        if word_2 in item: \n",
    "            value_2+=1\n",
    "        if word_1 in item and word_2 in item: \n",
    "            value_mutual+=1\n",
    "            #print(word_1)\n",
    "            #print(word_2)\n",
    "            #print(item)\n",
    "        count_word=count_word+item.count(' ')+1 #用空格作为分词的依据\n",
    "    #value_1= value_1/count_word\n",
    "    #value_2= value_2/count_word  \n",
    "    #value_mutual=value_mutual/count_word\n",
    "    if value_mutual != 0:\n",
    "        #value_pmi=log((value_mutual/(value_1*value_2)),2)   #原始pmi\n",
    "        value_pmi=value_mutual/(max(value_1,value_2))\n",
    "    else:\n",
    "        value_pmi=0 \n",
    "    return value_pmi    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#工具函数算边\n",
    "def GetBrim(list_word):\n",
    "    #获取节点间的边信息，主要使用相似度\n",
    "    list_link_dic=[]\n",
    "    global threshold_similarity  #全局变量，边的门限\n",
    "    model = models.word2vec.Word2Vec.load('word2vec.model') \n",
    "    for i in range(0,len(list_word)):\n",
    "        dic={}\n",
    "        for j in range(i+1,len(list_word)):\n",
    "            try:\n",
    "               similarity = model.wv.similarity(list_word[i],list_word[j])\n",
    "            except:\n",
    "               similarity=0   #主要是不存在某个词的情况\n",
    "            if(similarity>=0.8):  #判断点的权重,感觉不判断，只取边的前多少条反而比较好\n",
    "                dic={}\n",
    "                dic['Source']=i\n",
    "                dic['Target']=j\n",
    "                dic['Type']=0\n",
    "                dic['Weight']=similarity\n",
    "                #print(f\"边为{dic}\")\n",
    "                list_link_dic.append(dic)\n",
    "    print('\\n完成算边')\n",
    "    list_link_dic = sorted(list_link_dic, key=lambda x: x['Weight'],reverse=True)\n",
    "    return list_link_dic   #取前面的边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#工具函数 选择相关文档\n",
    "def enlargeWord(event_word,doclist):\n",
    "    global proportion,data_source\n",
    "    for item in event_word:\n",
    "      word_list=item['word']\n",
    "      doc_list=[]\n",
    "      #print(word_list)\n",
    "      for doc in doclist:\n",
    "          text = doc['data'] \n",
    "          text = [word for word in text.split() if word.strip('#') != \"\" or word.strip('@') != \"\"]  # 以空格为单位分词 \n",
    "          count=0   #统计词数\n",
    "          for single_word in text:\n",
    "              if(single_word in word_list): \n",
    "                  count+=1\n",
    "          if(count/len(word_list)>=proportion and word_list[0] in text):  #占比超过多少的视为相关文档,初始化设置为0.3. 并且需要包含核心词\n",
    "              #if(doc['like_count']+doc['retweet_count']*3>=5): #过滤无用户行为的\n",
    "                    doc_list.append(doc)\n",
    "      item['doc']= doc_list\n",
    "    return  event_word   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#工具函数  过滤事件\n",
    "def filteEvent(event_final):\n",
    "    global filter_word\n",
    "    event_return=[]\n",
    "    for item in event_final:\n",
    "        flag=0\n",
    "        #print('开始过滤')\n",
    "        #print(item['word'])\n",
    "        for word in item['word']:\n",
    "            #print(word)\n",
    "            if (word  in filter_word):\n",
    "                flag=1\n",
    "        if(flag==0):\n",
    "            event_return.append(item)\n",
    "    return event_return     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeyWord(doclist):\n",
    "    global data_source,delete_word,word_number,canditate_word\n",
    "    word_list_now=[]\n",
    "    kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "    list_word=[]\n",
    "    list_weight=[]\n",
    "    result={}\n",
    "    for doc in doclist:\n",
    "        item = doc['data']   \n",
    "        keywords_phrase = kw_model.extract_keywords(item, \n",
    "                                     keyphrase_ngram_range=(1,1),   #这个就是范围,可以考虑词组,本实验暂时只考虑词\n",
    "                                     stop_words='english', \n",
    "                                     highlight=False, \n",
    "                                     top_n=word_number) \n",
    "        keywords_phrase_list= list(keywords_phrase)#结果，返回的是列表包含元组的形式[(),()]\n",
    "        #print(keywords_phrase_list)\n",
    "        \n",
    "        for word in keywords_phrase_list:\n",
    "            #调整每个文件的屏蔽词\n",
    "            #if(word[0] not in filter_word):\n",
    "              if word[0] not in list_word:\n",
    "                list_word.append(word[0])\n",
    "                list_weight.append(word[1])\n",
    "              else:\n",
    "                index=list_word.index(word[0])\n",
    "                list_weight[index]=list_weight[index]+word[1]\n",
    "    result['word']=list_word\n",
    "    result['weight']=list_weight\n",
    "    sorted_word = sorted(result['word'], key=lambda x: result['weight'][result['word'].index(x)],reverse=True)[:canditate_word]  #根据weight操作word排序\n",
    "    sorted_weight=sorted(result['weight'],reverse=True)[:canditate_word]\n",
    "\n",
    "    #print(f\"提取的关键词为{sorted_word}\")\n",
    "    word_list=[]\n",
    "    for i in range(len(sorted_word)):\n",
    "        dic={}\n",
    "        dic['id']=i\n",
    "        dic['word']=sorted_word[i]\n",
    "        dic['weight']=sorted_weight[i]\n",
    "        word_list.append(dic)     #候选词的列表，每个元素为字典          \n",
    "    list_link_dic=GetBrim(sorted_word)   \n",
    "\n",
    "    #print(f\"边的数量为{len(list_link_dic)}\")\n",
    "    #聚类\n",
    "    G = nx.Graph()\n",
    "    for word in word_list:\n",
    "        G.add_node(word['id'],size = word['weight'])\n",
    "    for link in list_link_dic:   \n",
    "        G.add_edge(link['Source'],link['Target'],weight=link['Weight'])\n",
    "        #print(f\"边为{link}\")  \n",
    "    communities = nx.algorithms.community.girvan_newman(G)\n",
    "    best_communities = None\n",
    "    best_modularity = -1\n",
    "    for community in communities: #获取最佳聚类结果\n",
    "        #print(community)\n",
    "        Q = modularity(G, community)  #用以衡量聚类效果的函数\n",
    "        if Q > best_modularity:\n",
    "            best_modularity = Q \n",
    "            best_communities = community \n",
    "    list_community=[]  #存所有簇    \n",
    "    list_derail=[]  #存离群点\n",
    "    for i, community in enumerate(best_communities):\n",
    "        node=list(community)#字典转list\n",
    "        if len(community)==1:   #对于离群点\n",
    "            list_derail.append(node[0])\n",
    "        if len(community)<=3 and len(community)>1:   #对于小群\n",
    "            list_community.append(node)\n",
    "        if len(community)>3:\n",
    "            list_community.append(node)\n",
    "    #list_community.append(list_derail)    #最后一个一定是离群点集合\n",
    "    #print(list_community)\n",
    "    for event in list_community:\n",
    "        ls_event_word=[]\n",
    "        flag=0   #标记type\n",
    "        for i in event:  #遍历事件中每一个单词\n",
    "            for word in  word_list:\n",
    "                if(word['id']==i): \n",
    "                    word['type']=flag  #为每个词标记所属事件\n",
    "                    ls_event_word.append(word['word'])\n",
    "        flag+=1            \n",
    "        #print(f\"当前type关键词:{ls_event_word}\")\n",
    "    #为离群的词添加\n",
    "    derail_value = 1001\n",
    "    for i in list_derail:\n",
    "        for word in  word_list:\n",
    "                if(word['id']==i): \n",
    "                    word['type']=derail_value  #为每个词标记所属事件\n",
    "    return  word_list                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvent(word_list,doclist):\n",
    "    global data_source,delete_word,word_number\n",
    "    derail_value = 1001\n",
    "    event_word=[]\n",
    "    for i in range(len(word_list)):\n",
    "        for j in range(i+1,len(word_list)):\n",
    "            #同话题，不考虑离群\n",
    "            if(word_list[i]['type']==word_list[j]['type'] and word_list[i]['type']!=derail_value):\n",
    "                value_pmi=GetPointwiseMutualValue(word_list[i]['word'],word_list[j]['word'],doclist)\n",
    "                if(value_pmi>=0.1):    #话题增益\n",
    "                    keyword = max([word_list[i], word_list[j]], key=lambda x: x['weight'])\n",
    "                    if(keyword not in event_word):\n",
    "                        event_word.append(keyword)  #存核心词,然后再看第二步\n",
    "    event_final=[]\n",
    "    keyword_list=[]\n",
    "    for keyword in event_word:\n",
    "        keyword_list.append(keyword['word'])\n",
    "    for keyword in event_word:\n",
    "        dic={}\n",
    "        dic['keyword']=keyword\n",
    "        #print(f'核心词{keyword}')\n",
    "        dic['word']=[]\n",
    "        dic['word'].append(keyword['word'])#先把keyword本身的词加进去\n",
    "        dic['flag']=0\n",
    "        for word in word_list:\n",
    "            #if(word['word'] not in keyword_list):\n",
    "            if(word['word'] != keyword['word']):\n",
    "                pmi = GetPointwiseMutualValue(keyword['word'],word['word'],doclist)\n",
    "                if(pmi>=0.05 ):  #看pmi值\n",
    "                    if(keyword['type'] == word['type'] or word['type'] == derail_value):   #存一个type的 \n",
    "                        #print(keyword['word'],keyword['type'],word['word'],word['type'],pmi)\n",
    "                        dic['word'].append(word['word'])\n",
    "                        #print(word['word'])\n",
    "                        #print(pmi)\n",
    "        event_final.append(dic)                 \n",
    "    event_final = enlargeWord(event_final,doclist)\n",
    "\n",
    "    #处理可能存在的重复问题。  如果一个事件的keyword在另一个事件的word中，则合并\n",
    "    event_final_empty=[]\n",
    "    for i in range(len(event_final)):\n",
    "        #print(event_final[i])\n",
    "        for j in range(i+1,len(event_final)):\n",
    "            if(event_final[i]['keyword']['word'] in event_final[j]['word'] or event_final[j]['keyword']['word'] in event_final[i]['word']):   #判断keyword在word列表中的情况\n",
    "                event_final[i]['word']=list(set(event_final[i]['word']+event_final[j]['word']))\n",
    "                event_final[j]['flag']=1\n",
    "        if(event_final[i]['flag']==0):\n",
    "           event_final_empty.append(event_final[i])\n",
    "    event_final = event_final_empty\n",
    "    #过滤event\n",
    "    event_final = filteEvent(event_final)\n",
    "    event_return=[]\n",
    "    for item in event_final:\n",
    "        if(len(item['doc'])>5):\n",
    "            event_return.append(item)\n",
    "    return  event_return        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#工具函数，获得全局需要删除的高频词，只运行一次\n",
    "#需要记录结果\n",
    "def getGloablWord():\n",
    "    global delete_word,word_number\n",
    "    kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "    doclist=[]\n",
    "    with open('../推特GPT/GPT.csv',encoding=\"utf-8\") as f:\n",
    "         f_csv = csv.reader(f)\n",
    "         next(f_csv)\n",
    "         pos = 1\n",
    "         for row in f_csv:\n",
    "            if  len(row)>=6:  #控制聚类文本数目并处理缺失数据,全部跑太多了跑不了\n",
    "                #print(row[0][:10])  #取到日期\n",
    "                    doclist.append(row[2])\n",
    "                    pos+=1\n",
    "         print(f'总数量{pos}')\n",
    "    list_word=[]\n",
    "    list_weight=[]\n",
    "    result={}     \n",
    "    progress_bar=tqdm.tqdm(total=len(doclist))\n",
    "    for doc in doclist:\n",
    "        item = doc \n",
    "        keywords_phrase = kw_model.extract_keywords(item, \n",
    "                                     keyphrase_ngram_range=(1,1),   #这个就是范围,可以考虑词组,本实验暂时只考虑词\n",
    "                                     stop_words='english', \n",
    "                                     highlight=False, \n",
    "                                     top_n=word_number) \n",
    "        keywords_phrase_list= list(keywords_phrase)#结果，返回的是列表包含元组的形式[(),()]\n",
    "        #print(keywords_phrase_list)\n",
    "        \n",
    "        for word in keywords_phrase_list:\n",
    "          if word[0] not in delete_word[:10]:         \n",
    "              if word[0] not in list_word:\n",
    "                list_word.append(word[0])\n",
    "                list_weight.append(word[1])\n",
    "              else:\n",
    "                index=list_word.index(word[0])\n",
    "                list_weight[index]=list_weight[index]+word[1]\n",
    "        progress_bar.update(1)        \n",
    "    result['word']=list_word\n",
    "    result['weight']=list_weight\n",
    "    sorted_word = sorted(result['word'], key=lambda x: result['weight'][result['word'].index(x)],reverse=True)  #根据weight操作word排序\n",
    "    sorted_weight=sorted(result['weight'],reverse=True)\n",
    "   \n",
    "    #print(sorted_word)\n",
    "    #print(sorted_weight)\n",
    "    return sorted_word,sorted_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_word=['chatgpt', 'gpt', 'ai', 'chat', 'openai', 'artificialintelligence', 'chatbot', 'google', 'bing', 'https','data','language']\n",
    "#filter_word=[]  #不要全局的增益\n",
    "proportion = 0.4\n",
    "#记录一个问题，这种数据如果训练词向量的时候维度太高，就不好用相似度解开\n",
    "random.seed(42)\n",
    "#date_list=['2022-01-01','2022-01-02','2022-01-03','2022-01-04','2022-01-05']\n",
    "date_list=[\n",
    "    ['2023-01-06','2023-01-07','2023-01-08'],\n",
    "    ['2023-01-09','2023-01-10','2023-01-11'],\n",
    "    ['2023-01-12','2023-01-13','2023-01-14'],\n",
    "    ['2023-01-15','2023-01-16','2023-01-17'],\n",
    "    ['2023-01-18','2023-01-19','2023-01-20'],\n",
    "    ['2023-01-21','2023-01-22','2023-01-23'],\n",
    "    ['2023-01-24','2023-01-25','2023-01-26'],\n",
    "    ['2023-01-27','2023-01-28','2023-01-29'],\n",
    "    ['2023-01-30','2023-01-31','2023-02-01'],\n",
    "    ['2023-02-02','2023-02-03','2023-02-04'],\n",
    "    ['2023-02-05','2023-02-06','2023-02-07'],\n",
    "    ['2023-02-08','2023-02-09','2023-02-10'],\n",
    "    ['2023-02-11','2023-02-12','2023-02-13'],\n",
    "    ['2023-02-14','2023-02-15','2023-02-16'],\n",
    "    ['2023-02-17','2023-02-18','2023-02-19'],\n",
    "    ['2023-02-20','2023-02-21','2023-02-22'],\n",
    "    ['2023-02-23','2023-02-24','2023-02-25'],\n",
    "    ['2023-02-26','2023-02-27','2023-02-28'],\n",
    "]\n",
    "word_number=3\n",
    "canditate_word = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_event_list=['education', 'teachers','student','students','teacher','school', 'writing', 'write','schools', 'essay','essays','tweet', 'tweets','bitcoin', 'crypto','prompts', 'prompt'\n",
    "'cybersecurity', 'malware','exam', 'exams','bias', 'biased','marketing', 'business','market', 'phishing','pish','generated','jobs','job','billion','worth','value','poem','art']\n",
    "real_event_word=[\n",
    "    ['students','teachers','schools','education','NYC'],\n",
    "    ['student','teacher','school','education','NYC'],\n",
    "    ['tweet','generated'],\n",
    "    ['write','essay','poem','art'],\n",
    "    ['cybersecurity','pish','malware'],\n",
    "    ['exam'],\n",
    "    ['bias'],\n",
    "    ['valentine'],\n",
    "    ['jobs','replace'],\n",
    "    ['bitcoin','crypto','marketing','business'],\n",
    "    ['write','prompt'],\n",
    "    ['billion','worth','value']\n",
    "]\n",
    "real_event_length=27\n",
    "result=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日期为['2023-01-06', '2023-01-07', '2023-01-08']\n",
      "\n",
      "完成算边\n",
      "词总数为60\n",
      "['essay', 'education', 'writing', 'teachers', 'students', 'schools']\n",
      "['tweet', 'tweets']\n",
      "['startups', 'youtube', 'valuation', 'automation']\n",
      "日期为['2023-01-09', '2023-01-10', '2023-01-11']\n",
      "\n",
      "完成算边\n",
      "词总数为74\n",
      "['writing', 'text', 'code', 'essay', 'content', 'write']\n",
      "['marketing', 'seo', 'business', 'content']\n",
      "['essay', 'education', 'essays', 'twitter', 'students', 'teachers', 'schools']\n",
      "['tweet', 'tweets']\n",
      "日期为['2023-01-12', '2023-01-13', '2023-01-14']\n",
      "\n",
      "完成算边\n",
      "词总数为71\n",
      "['prompts', 'prompt']\n",
      "['tweet', 'tweets']\n",
      "日期为['2023-01-15', '2023-01-16', '2023-01-17']\n",
      "\n",
      "完成算边\n",
      "词总数为74\n",
      "['code', 'writing', 'nick', 'content', 'search', 'write', 'song', 'prompt', 'nickcave']\n",
      "['teaching', 'essay', 'education', 'essays', 'twitter', 'students', 'teachers', 'write']\n",
      "['tweet', 'tweets']\n",
      "['youtube', 'automation']\n",
      "日期为['2023-01-18', '2023-01-19', '2023-01-20']\n",
      "\n",
      "完成算边\n",
      "词总数为73\n",
      "['prompts', 'prompt']\n",
      "日期为['2023-01-21', '2023-01-22', '2023-01-23']\n",
      "\n",
      "完成算边\n",
      "词总数为72\n",
      "['tweet', 'tweets']\n",
      "日期为['2023-01-24', '2023-01-25', '2023-01-26']\n",
      "\n",
      "完成算边\n",
      "词总数为73\n",
      "['marketing', 'content', 'business']\n",
      "['prompts', 'prompt', 'promptengineering']\n",
      "['tweet', 'tweets']\n",
      "日期为['2023-01-27', '2023-01-28', '2023-01-29']\n",
      "\n",
      "完成算边\n",
      "词总数为71\n",
      "['prompts', 'prompt']\n",
      "日期为['2023-01-30', '2023-01-31', '2023-02-01']\n",
      "\n",
      "完成算边\n",
      "词总数为72\n",
      "['writing', 'content', 'prompt', 'copywriting', 'write']\n",
      "['essay', 'education', 'exam', 'students', 'teachers']\n",
      "日期为['2023-02-02', '2023-02-03', '2023-02-04']\n",
      "\n",
      "完成算边\n",
      "词总数为76\n",
      "日期为['2023-02-05', '2023-02-06', '2023-02-07']\n",
      "\n",
      "完成算边\n",
      "词总数为76\n",
      "['tweet', 'tweets']\n",
      "['robots', 'robot']\n",
      "日期为['2023-02-08', '2023-02-09', '2023-02-10']\n",
      "\n",
      "完成算边\n",
      "词总数为73\n",
      "['prompt', 'writing', 'text', 'prompts']\n",
      "日期为['2023-02-11', '2023-02-12', '2023-02-13']\n",
      "\n",
      "完成算边\n",
      "词总数为79\n",
      "日期为['2023-02-14', '2023-02-15', '2023-02-16']\n",
      "\n",
      "完成算边\n",
      "词总数为79\n",
      "['prompts', 'chatgpt3', 'writing', 'prompt']\n",
      "日期为['2023-02-17', '2023-02-18', '2023-02-19']\n",
      "\n",
      "完成算边\n",
      "词总数为76\n",
      "日期为['2023-02-20', '2023-02-21', '2023-02-22']\n",
      "\n",
      "完成算边\n",
      "词总数为74\n",
      "['prompts', 'prompt']\n",
      "日期为['2023-02-23', '2023-02-24', '2023-02-25']\n",
      "\n",
      "完成算边\n",
      "词总数为75\n",
      "['malware', 'cybersecurity', 'phishing']\n",
      "日期为['2023-02-26', '2023-02-27', '2023-02-28']\n",
      "\n",
      "完成算边\n",
      "词总数为75\n",
      "['prompt', 'writing', 'prompts', 'text']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "word_list_pre=[]\n",
    "for item in date_list:\n",
    "        print(f\"日期为{item}\")\n",
    "        doclist=readData(item)\n",
    "        #print(len(doclist))\n",
    "        word_list=getKeyWord(doclist)\n",
    "        word_list_now=word_list.copy()  #用以暂存这次的,注意深复制\n",
    "        for word_pre in word_list_pre:\n",
    "            flag=0\n",
    "            for item in word_list:\n",
    "                if(item['word']==word_pre['word']):\n",
    "                    flag=1\n",
    "            if(flag==0):\n",
    "                word_list.append(word_pre)\n",
    "        print(f\"词总数为{len(word_list)}\")        \n",
    "        event_final=getEvent(word_list,doclist)\n",
    "        for event in event_final[:50]:\n",
    "               #print(event['keyword']['word'],event['word'])\n",
    "                print(event['word'])\n",
    "                result.append(event['word'])\n",
    "                #for doc in event['doc'][:50]:\n",
    "                #    print(doc)\n",
    "\n",
    "        word_list_pre = word_list_now            \n",
    "end_time = time.time()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获得的事件为['essay', 'education', 'writing', 'teachers', 'students', 'schools']\n",
      "归属于事件['students', 'teachers', 'schools', 'education', 'NYC']\n",
      "获得的事件为['tweet', 'tweets']\n",
      "归属于事件['tweet', 'generated']\n",
      "获得的事件为['startups', 'youtube', 'valuation', 'automation']\n",
      "获得的事件为['writing', 'text', 'code', 'essay', 'content', 'write']\n",
      "归属于事件['write', 'essay', 'poem', 'art']\n",
      "获得的事件为['marketing', 'seo', 'business', 'content']\n",
      "归属于事件['bitcoin', 'crypto', 'marketing', 'business']\n",
      "获得的事件为['essay', 'education', 'essays', 'twitter', 'students', 'teachers', 'schools']\n",
      "归属于事件['students', 'teachers', 'schools', 'education', 'NYC']\n",
      "获得的事件为['tweet', 'tweets']\n",
      "归属于事件['tweet', 'generated']\n",
      "获得的事件为['prompts', 'prompt']\n",
      "归属于事件['write', 'prompt']\n",
      "获得的事件为['tweet', 'tweets']\n",
      "归属于事件['tweet', 'generated']\n",
      "获得的事件为['code', 'writing', 'nick', 'content', 'search', 'write', 'song', 'prompt', 'nickcave']\n",
      "归属于事件['write', 'prompt']\n",
      "获得的事件为['teaching', 'essay', 'education', 'essays', 'twitter', 'students', 'teachers', 'write']\n",
      "归属于事件['students', 'teachers', 'schools', 'education', 'NYC']\n",
      "获得的事件为['tweet', 'tweets']\n",
      "归属于事件['tweet', 'generated']\n",
      "获得的事件为['youtube', 'automation']\n",
      "获得的事件为['prompts', 'prompt']\n",
      "归属于事件['write', 'prompt']\n",
      "获得的事件为['tweet', 'tweets']\n",
      "归属于事件['tweet', 'generated']\n",
      "获得的事件为['marketing', 'content', 'business']\n",
      "归属于事件['bitcoin', 'crypto', 'marketing', 'business']\n",
      "获得的事件为['prompts', 'prompt', 'promptengineering']\n",
      "归属于事件['write', 'prompt']\n",
      "获得的事件为['tweet', 'tweets']\n",
      "归属于事件['tweet', 'generated']\n",
      "获得的事件为['prompts', 'prompt']\n",
      "归属于事件['write', 'prompt']\n",
      "获得的事件为['writing', 'content', 'prompt', 'copywriting', 'write']\n",
      "归属于事件['write', 'prompt']\n",
      "获得的事件为['essay', 'education', 'exam', 'students', 'teachers']\n",
      "归属于事件['students', 'teachers', 'schools', 'education', 'NYC']\n",
      "获得的事件为['tweet', 'tweets']\n",
      "归属于事件['tweet', 'generated']\n",
      "获得的事件为['robots', 'robot']\n",
      "获得的事件为['prompt', 'writing', 'text', 'prompts']\n",
      "归属于事件['write', 'prompt']\n",
      "获得的事件为['prompts', 'chatgpt3', 'writing', 'prompt']\n",
      "归属于事件['write', 'prompt']\n",
      "获得的事件为['prompts', 'prompt']\n",
      "归属于事件['write', 'prompt']\n",
      "获得的事件为['malware', 'cybersecurity', 'phishing']\n",
      "归属于事件['cybersecurity', 'pish', 'malware']\n",
      "获得的事件为['prompt', 'writing', 'prompts', 'text']\n",
      "归属于事件['write', 'prompt']\n",
      "全部词为['promptengineering', 'automation', 'nick', 'copywriting', 'prompt', 'robot', 'nickcave', 'malware', 'text', 'essay', 'writing', 'phishing', 'seo', 'business', 'prompts', 'teaching', 'education', 'essays', 'startups', 'twitter', 'tweets', 'teachers', 'write', 'song', 'chatgpt3', 'cybersecurity', 'tweet', 'code', 'robots', 'youtube', 'exam', 'content', 'search', 'students', 'schools', 'marketing', 'valuation']\n",
      "召回词为['tweet', 'essay', 'education', 'writing', 'essays', 'exam', 'phishing', 'tweets', 'teachers', 'students', 'write', 'business', 'schools', 'marketing', 'prompts', 'malware']\n",
      "事件正确率0.8928571428571429\n",
      "召回词数16\n",
      "召回事件\n",
      "词正确率为0.43243243243243246\n",
      "运行时间为7374.510762929916\n"
     ]
    }
   ],
   "source": [
    "all_word=[]\n",
    "get_real_word=[]\n",
    "count_recall_event=0\n",
    "for event in result:\n",
    "    # 使用逗号分割字符串，并去除空格\n",
    "    words = event\n",
    "    print(f\"获得的事件为{words}\")\n",
    "    for word in words:\n",
    "        if(word in real_event_list):\n",
    "            #print(word)\n",
    "            get_real_word.append(word)\n",
    "    flag_belong_event=0        \n",
    "    for real_event_this in real_event_word:\n",
    "        count_belong_this=0\n",
    "        for real_word_this in real_event_this :        \n",
    "            if(real_word_this in words):\n",
    "                count_belong_this+=1\n",
    "        if(count_belong_this>len(real_event_this)*0.4 and flag_belong_event==0): \n",
    "            count_recall_event+=1\n",
    "            flag_belong_event =1  #置1，只能属于一个事件\n",
    "            print(f\"归属于事件{real_event_this}\")\n",
    "    all_word.extend(words) \n",
    "all_word = list(set(all_word)) \n",
    "get_real_word = list(set(get_real_word))\n",
    "print(f\"全部词为{all_word}\")\n",
    "print(f'召回词为{get_real_word}')          \n",
    "print(f'事件正确率{count_recall_event/len(result)}')\n",
    "print(f'召回词数{len(get_real_word)}')\n",
    "print(f'召回事件')\n",
    "print(f'词正确率为{len(get_real_word)/len(all_word)}')\n",
    "print(f\"运行时间为{end_time - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Opinion_Zhou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
