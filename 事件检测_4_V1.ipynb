{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community #  \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "import random\n",
    "from keybert import KeyBERT\n",
    "import time\n",
    "from gensim import models,corpora\n",
    "from math import log, exp\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#工具函数\n",
    "#计算pmi\n",
    "def GetPointwiseMutualValue(word_1,word_2,doclist):\n",
    "    value_1=0\n",
    "    value_2=0\n",
    "    value_mutual=0\n",
    "    count_word=1  #一共出现多少词,加一不影响结果，只是防止除0\n",
    "    for item in doclist:\n",
    "        item=item['data']\n",
    "        if word_1 in item: \n",
    "            value_1+=1\n",
    "        if word_2 in item: \n",
    "            value_2+=1\n",
    "        if word_1 in item and word_2 in item: \n",
    "            value_mutual+=1\n",
    "            #print(word_1)\n",
    "            #print(word_2)\n",
    "            #print(item)\n",
    "        count_word=count_word+item.count(' ')+1 #用空格作为分词的依据\n",
    "    #value_1= value_1/count_word\n",
    "    #value_2= value_2/count_word  \n",
    "    #value_mutual=value_mutual/count_word\n",
    "    if value_mutual != 0:\n",
    "        #value_pmi=log((value_mutual/(value_1*value_2)),2)   #原始pmi\n",
    "        value_pmi=value_mutual/(max(value_1,value_2))\n",
    "    else:\n",
    "        value_pmi=0 \n",
    "    return value_pmi    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#工具函数  过滤事件\n",
    "def filteEvent(event_final):\n",
    "    global filter_word\n",
    "    event_return=[]\n",
    "    for item in event_final:\n",
    "        flag=0\n",
    "        #print('开始过滤')\n",
    "        #print(item['word'])\n",
    "        for word in item['word']:\n",
    "            #print(word)\n",
    "            if (word  in filter_word):\n",
    "                flag=1\n",
    "        if(flag==0):\n",
    "            event_return.append(item)\n",
    "    return event_return                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主要函数  1\n",
    "#从文件夹中读取type信息\n",
    "def getTypeWordAndData(file,datelist):\n",
    "    word_list=[]\n",
    "    with open(file+'-node_type.csv',encoding=\"utf-8\") as f:\n",
    "     f_csv = csv.reader(f)\n",
    "     next(f_csv)\n",
    "     for row in f_csv:\n",
    "        dic={}\n",
    "        dic['word']=row[1]\n",
    "        dic['size']=eval(row[2])\n",
    "        dic['type']=row[3]\n",
    "        #if(row[1] not in ['chatgpt', 'gpt', 'ai', 'chat', 'openai', 'artificialintelligence', 'chatbot', 'google', 'bing', 'https']):\n",
    "        word_list.append(dic)\n",
    "    doclist=[]\n",
    "    with open('../推特GPT/GPT.csv',encoding=\"utf-8\") as f:\n",
    "         f_csv = csv.reader(f)\n",
    "         pos = 1\n",
    "         for row in f_csv:\n",
    "            if  len(row)>=6:  #控制聚类文本数目并处理缺失数据,全部跑太多了跑不了\n",
    "                #print(row[0][:10])  #取到日期\n",
    "                if row[0][:10] in datelist: #判断时间\n",
    "                    #print(row[0])\n",
    "                    dic={}\n",
    "                    dic['like_count']=eval(row[4])\n",
    "                    dic['retweet_count']=eval(row[5])\n",
    "                    dic['data'] = row[2]\n",
    "                    doclist.append(dic)\n",
    "                    pos+=1  \n",
    "    return word_list,doclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主要函数  2   V1\n",
    "#从全部词中取得有联系的词\n",
    "def getPMIAndEventWord_v1(word_list,doclist):\n",
    "    derail_value = max(word_list, key=lambda d: d[\"type\"])[\"type\"]\n",
    "    event_word=[]\n",
    "    #print(word_list)\n",
    "    for i in range(len(word_list)):\n",
    "        for j in range(i+1,len(word_list)):\n",
    "            #同话题，不考虑离群\n",
    "            if(word_list[i]['type']==word_list[j]['type'] and word_list[i]['type']!=derail_value):\n",
    "                value_pmi=GetPointwiseMutualValue(word_list[i]['word'],word_list[j]['word'],doclist)\n",
    "                if(value_pmi>=0.05):    #话题增益\n",
    "                    dic={}\n",
    "                    dic['word']=[]\n",
    "                    dic['word'].append(word_list[i]['word'])\n",
    "                    dic['word'].append(word_list[j]['word'])\n",
    "                    dic['type']=word_list[i]['type']\n",
    "                    dic['pmi']=value_pmi\n",
    "                    dic['flag']=0\n",
    "                    dic['keyword'] = max([word_list[i], word_list[j]], key=lambda x: x['size'])\n",
    "                    event_word.append(dic)\n",
    "    event_word = sorted(event_word, key=lambda d: d[\"pmi\"],reverse=True)\n",
    "    event_final=[]\n",
    "    #为每个\n",
    "    #存在相同的词则合并\n",
    "    remove_word=['ai','https','msft']\n",
    "    #for i in range(len(event_word)):\n",
    "    #    dic={}\n",
    "    #    #print(event_word[i])\n",
    "    #    #print(event_word[i]['flag'])\n",
    "    #    if(event_word[i]['flag']==0):  #表示没有被操作过\n",
    "    #        for j in range(i+1,len(event_word)):\n",
    "    #            for j_word in event_word[j]['word']:\n",
    "    #                if(j_word in event_word[i]['word']):\n",
    "    #                    event_word[j]['flag']=1\n",
    "    #            if(event_word[j]['flag']==1):\n",
    "    #                event_word[i]['word']=event_word[i]['word']+event_word[j]['word']          \n",
    "    #                event_word[i]['word']=list(set(event_word[i]['word']))\n",
    "    #        event_word[i]['word'] = [x for x in event_word[i]['word'] if x not in remove_word]        \n",
    "    #        event_final.append(event_word[i])\n",
    "    #        #去除过于核心的词和没有意义的词\n",
    "    for i in range(len(event_word)):\n",
    "        dic={}\n",
    "        if(event_word[i]['flag']==0):  #表示没有被操作过\n",
    "            for j in range(i+1,len(event_word)):\n",
    "                if(event_word[i]['keyword']==event_word[j]['keyword']):\n",
    "                    event_word[j]['flag']=1  #标为可合并\n",
    "            if(event_word[j]['flag']==1):   \n",
    "                event_word[i]['word']=event_word[i]['word']+event_word[j]['word']          \n",
    "                event_word[i]['word']=list(set(event_word[i]['word']))\n",
    "            event_word[i]['word'] = [x for x in event_word[i]['word'] if x not in remove_word]        \n",
    "            event_final.append(event_word[i])     \n",
    "    #print(event_final)        \n",
    "    event_word = enlargeWord(event_final,doclist)\n",
    "    return event_word        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主要函数  2  \n",
    "#从全部词中取得有联系的词\n",
    "def getPMIAndEventWord(word_list,doclist):\n",
    "    derail_value = max(word_list, key=lambda d: d[\"type\"])[\"type\"]\n",
    "    event_word=[]\n",
    "    #print(word_list)\n",
    "    for i in range(len(word_list)):\n",
    "        for j in range(i+1,len(word_list)):\n",
    "            #同话题，不考虑离群\n",
    "            if(word_list[i]['type']==word_list[j]['type'] and word_list[i]['type']!=derail_value):\n",
    "                value_pmi=GetPointwiseMutualValue(word_list[i]['word'],word_list[j]['word'],doclist)\n",
    "                if(value_pmi>=0.1):    #话题增益\n",
    "                    keyword = max([word_list[i], word_list[j]], key=lambda x: x['size'])\n",
    "                    if(keyword not in event_word):\n",
    "                        event_word.append(keyword)  #存核心词,然后再看第二步\n",
    "    event_final=[]\n",
    "    for keyword in event_word:\n",
    "        dic={}\n",
    "        dic['keyword']=keyword\n",
    "        #print(f'核心词{keyword}')\n",
    "        dic['word']=[]\n",
    "        dic['word'].append(keyword['word'])\n",
    "        dic['flag']=0\n",
    "        for word in word_list:\n",
    "            if(word['word'] != keyword['word']):\n",
    "                pmi = GetPointwiseMutualValue(keyword['word'],word['word'],doclist)\n",
    "                if(pmi>=0.05 ):  #看pmi值\n",
    "                    if(keyword['type'] == word['type'] or word['type'] == derail_value):   #存一个type的 \n",
    "                        dic['word'].append(word['word'])\n",
    "        event_final.append(dic)                 \n",
    "    event_final = enlargeWord(event_final,doclist)\n",
    "    \n",
    "    #处理可能存在的重复问题。  如果一个事件的keyword在另一个事件的word中，则合并\n",
    "    event_final_empty=[]\n",
    "    for i in range(len(event_final)):\n",
    "        #print(event_final[i])\n",
    "        for j in range(i+1,len(event_final)):\n",
    "            if(event_final[i]['keyword']['word'] in event_final[j]['word'] or event_final[j]['keyword']['word'] in event_final[i]['word']):   #判断keyword在word列表中的情况\n",
    "                event_final[i]['word']=list(set(event_final[i]['word']+event_final[j]['word']))\n",
    "                event_final[j]['flag']=1\n",
    "        if(event_final[i]['flag']==0):\n",
    "           event_final_empty.append(event_final[i])\n",
    "    event_final = event_final_empty\n",
    "    #过滤event\n",
    "    event_final = filteEvent(event_final)                \n",
    "    return event_final        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主要函数  3\n",
    "#选择相关文档\n",
    "def enlargeWord(event_word,doclist):\n",
    "    global proportion\n",
    "    for item in event_word:\n",
    "      word_list=item['word']\n",
    "      doc_list=[]\n",
    "      #print(word_list)\n",
    "      for doc in doclist:\n",
    "          text=doc['data']\n",
    "          text = [word for word in text.split() if word.strip('#') != \"\" or word.strip('@') != \"\"]  # 以空格为单位分词 \n",
    "          count=0   #统计词数\n",
    "          for single_word in text:\n",
    "              if(single_word in word_list): \n",
    "                  count+=1\n",
    "          if(count/len(word_list)>=proportion):  #占比超过多少的视为相关文档,初始化设置为0.3\n",
    "              #if(doc['like_count']+doc['retweet_count']*3>=5): #过滤无用户行为的\n",
    "                    doc_list.append(doc)\n",
    "      item['doc']= doc_list\n",
    "    return  event_word             \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=[\n",
    "    '../推特GPT/成图csvV3/1月6日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/1月9日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/1月12日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/1月15日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/1月18日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/1月21日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/1月24日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/1月27日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/1月30日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/2月2日起11728全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/2月5日起22806全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/2月8日起24368全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/2月11日起17814全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/2月14日起18673全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/2月17日起16115全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/2月20日起17954全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/2月23日起15076全数据，边为相似度,门限0.8',\n",
    "    '../推特GPT/成图csvV3/2月26日起13956全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月1日起17547全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月4日起13555全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月7日起15353全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月10日起12912全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月13日起25126全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月16日起25263全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月19日起20431全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月22日起24489全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月25日起19776全数据，边为相似度,门限0.8',\n",
    "    #'../推特GPT/成图csvV3/3月28日起16429全数据，边为相似度,门限0.8',\n",
    "]\n",
    "date_list=[\n",
    "    ['2023-01-06','2023-01-07','2023-01-08'],\n",
    "    ['2023-01-09','2023-01-10','2023-01-11'],\n",
    "    ['2023-01-12','2023-01-13','2023-01-14'],\n",
    "    ['2023-01-15','2023-01-16','2023-01-17'],\n",
    "    ['2023-01-18','2023-01-19','2023-01-20'],\n",
    "    ['2023-01-21','2023-01-22','2023-01-23'],\n",
    "    ['2023-01-24','2023-01-25','2023-01-26'],\n",
    "    ['2023-01-27','2023-01-28','2023-01-29'],\n",
    "    ['2023-01-30','2023-01-31','2023-02-01'],\n",
    "    ['2023-02-02','2023-02-03','2023-02-04'],\n",
    "    ['2023-02-05','2023-02-06','2023-02-07'],\n",
    "    ['2023-02-08','2023-02-09','2023-02-10'],\n",
    "    ['2023-02-11','2023-02-12','2023-02-13'],\n",
    "    ['2023-02-14','2023-02-15','2023-02-16'],\n",
    "    ['2023-02-17','2023-02-18','2023-02-19'],\n",
    "    ['2023-02-20','2023-02-21','2023-02-22'],\n",
    "    ['2023-02-23','2023-02-24','2023-02-25'],\n",
    "    ['2023-02-26','2023-02-27','2023-02-28'],\n",
    "    #['2023-03-01','2023-03-02','2023-03-03'],\n",
    "    #['2023-03-04','2023-03-05','2023-03-06'],\n",
    "    #['2023-03-07','2023-03-08','2023-03-09'],\n",
    "    #['2023-03-10','2023-03-11','2023-03-12'],\n",
    "    #['2023-03-13','2023-03-14','2023-03-15'],\n",
    "    #['2023-03-16','2023-03-17','2023-03-18'],\n",
    "    #['2023-03-19','2023-03-20','2023-03-21'],\n",
    "    #['2023-03-22','2023-03-23','2023-03-24'],\n",
    "    #['2023-03-25','2023-03-26','2023-03-27'],\n",
    "    #['2023-03-28','2023-03-29','2023-03-30'],\n",
    "]\n",
    "proportion = 0.4  #事件中有多少个词出现在一条文本中，则将该文本视为该事件的文本。  该值为门限比例，有超过这个比例的词出现则视为True     \n",
    "\n",
    "#从全数据中得到需要去除的词\n",
    "filter_word=['chatgpt', 'gpt', 'ai', 'chat', 'openai', 'artificialintelligence', 'chatbot', 'google', 'bing', 'https','data','language']\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全时间段   MyModule最终结果评估\n",
    "召回率 = 22/27\n",
    "正确率 = 76/94    #预测出的词有多少对了\n",
    "#混淆率 = 5/94     #预测出的词有多少是其他事件的,这个不是很好\n",
    "事件召回率 = 10/11\n",
    "事件正确率 = 36/39      #预测出的事件有不少于40%词属于某一个确定事件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023-01-06', '2023-01-07', '2023-01-08']\n",
      "新事件:['education', 'teachers', 'writing', 'schools', 'essay', 'students']\n",
      "共有文档:65\n",
      "新事件:['tweet', 'tweets']\n",
      "共有文档:130\n",
      "['2023-01-09', '2023-01-10', '2023-01-11']\n",
      "新事件:['writing', 'essay', 'content']\n",
      "共有文档:83\n",
      "新事件:['students', 'teachers', 'education']\n",
      "共有文档:33\n",
      "新事件:['essays', 'essay']\n",
      "共有文档:116\n",
      "['2023-01-12', '2023-01-13', '2023-01-14']\n",
      "新事件:['essays', 'poem', 'essay']\n",
      "共有文档:13\n",
      "新事件:['tweet', 'tweets']\n",
      "共有文档:117\n",
      "['2023-01-15', '2023-01-16', '2023-01-17']\n",
      "新事件:['students', 'teachers', 'education']\n",
      "共有文档:44\n",
      "新事件:['essay', 'essays']\n",
      "共有文档:102\n",
      "['2023-01-18', '2023-01-19', '2023-01-20']\n",
      "新事件:['prompts', 'prompt']\n",
      "共有文档:216\n",
      "['2023-01-21', '2023-01-22', '2023-01-23']\n",
      "新事件:['bitcoin', 'crypto']\n",
      "共有文档:32\n",
      "新事件:['students', 'teachers', 'essay']\n",
      "共有文档:32\n",
      "新事件:['cybersecurity', 'malware']\n",
      "共有文档:25\n",
      "新事件:['prompts', 'prompt']\n",
      "共有文档:198\n",
      "新事件:['tweet', 'tweets']\n",
      "共有文档:123\n",
      "['2023-01-24', '2023-01-25', '2023-01-26']\n",
      "新事件:['exam', 'exams']\n",
      "共有文档:235\n",
      "新事件:['marketing', 'content', 'business']\n",
      "共有文档:70\n",
      "新事件:['jobs', 'job']\n",
      "共有文档:282\n",
      "新事件:['education', 'students']\n",
      "共有文档:293\n",
      "新事件:['prompts', 'prompt']\n",
      "共有文档:244\n",
      "新事件:['tweets', 'tweet']\n",
      "共有文档:153\n",
      "['2023-01-27', '2023-01-28', '2023-01-29']\n",
      "新事件:['students', 'writing', 'education', 'teachers']\n",
      "共有文档:54\n",
      "新事件:['exam', 'exams']\n",
      "共有文档:129\n",
      "新事件:['prompt', 'writing', 'text', 'prompts']\n",
      "共有文档:61\n",
      "['2023-01-30', '2023-01-31', '2023-02-01']\n",
      "新事件:['students', 'education', 'teachers', 'exam']\n",
      "共有文档:47\n",
      "新事件:['prompts', 'prompt']\n",
      "共有文档:212\n",
      "['2023-02-02', '2023-02-03', '2023-02-04']\n",
      "新事件:['bias', 'biased', 'woke']\n",
      "共有文档:34\n",
      "新事件:['marketing', 'business', 'content']\n",
      "共有文档:71\n",
      "新事件:['prompts', 'prompt']\n",
      "共有文档:208\n",
      "['2023-02-05', '2023-02-06', '2023-02-07']\n",
      "['2023-02-08', '2023-02-09', '2023-02-10']\n",
      "['2023-02-11', '2023-02-12', '2023-02-13']\n",
      "['2023-02-14', '2023-02-15', '2023-02-16']\n",
      "新事件:['valentine', 'valentines']\n",
      "共有文档:34\n",
      "新事件:['marketing', 'seo', 'business', 'content']\n",
      "共有文档:58\n",
      "['2023-02-17', '2023-02-18', '2023-02-19']\n",
      "新事件:['musk', 'elon']\n",
      "共有文档:13\n",
      "['2023-02-20', '2023-02-21', '2023-02-22']\n",
      "新事件:['crypto', 'web3']\n",
      "共有文档:70\n",
      "新事件:['prompts', 'prompt']\n",
      "共有文档:219\n",
      "['2023-02-23', '2023-02-24', '2023-02-25']\n",
      "新事件:['trading', 'robots', 'invest', 'stocks']\n",
      "共有文档:7\n",
      "新事件:['malware', 'cybersecurity', 'phishing']\n",
      "共有文档:7\n",
      "新事件:['prompts', 'prompt']\n",
      "共有文档:206\n",
      "['2023-02-26', '2023-02-27', '2023-02-28']\n",
      "新事件:['musk', 'elon']\n",
      "共有文档:8\n",
      "新事件:['prompts', 'writing', 'prompt', 'text']\n",
      "共有文档:43\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(file_list)):\n",
    "    print(date_list[i])\n",
    "    word_list,doclist = getTypeWordAndData(file_list[i],date_list[i])\n",
    "    word_list_now=word_list.copy()  #用以暂存这次的,注意深复制\n",
    "    if(i != 0):\n",
    "        #word_list=word_list+word_list_pre\n",
    "        #将上一次的词加入本次word\n",
    "        for word_pre in word_list_pre:\n",
    "            flag=0\n",
    "            for item in word_list:\n",
    "                if(item['word']==word_pre['word']):\n",
    "                    flag=1\n",
    "            if(flag==0):\n",
    "                word_list.append(word_pre)        \n",
    "    event_get = getPMIAndEventWord(word_list,doclist)  #得到的事件\n",
    "    for item in event_get:\n",
    "        \n",
    "        #for doc in item['doc']:\n",
    "        #    print(doc['data'])\n",
    "        if(len(item['doc'])>5):   #认为检测出的事件显著\n",
    "            print(f\"新事件:{item['word']}\")\n",
    "            print(f\"共有文档:{len(item['doc'])}\")\n",
    "            #for doc in item['doc'][:50]:  #只取前20个\n",
    "            #    print(doc)\n",
    "    word_list_pre = word_list_now    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
